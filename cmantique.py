# -*- coding: utf-8 -*-
"""cmantique.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dumRFjtjDUJOcoIeotD_7YfvFqRHw6n2

---
# Packages to install :
"""

!pip install transformers sentencepiece

"""---
# Import :
"""

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from transformers import TFCamembertModel, CamembertTokenizer
import numpy as np
from tqdm import tqdm
import pandas as pd
from google.colab import drive, files

"""---
# Load tokenizer & Camembert :
"""

tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
camembert = TFCamembertModel.from_pretrained("camembert-base")

"""---
# Load data and reshape :
"""

drive.mount("./drive")

df = pd.read_csv("/content/drive/MyDrive/cmantique/semFr.csv")

df.head()

df = df.dropna(axis=0)

df.describe()

s1, s2 = df["sentence1"].values.tolist(), df["sentence2"].values.tolist()
labels = df["label"].values.tolist()

tokenized_s1 = [tokenizer(sentence, max_length=128, padding="max_length")["input_ids"] for sentence in tqdm(s1)]
tokenized_s2 = [tokenizer(sentence, max_length=128, padding="max_length")["input_ids"] for sentence in tqdm(s2)]

labels = list(map(lambda element: element+1, labels))

encoded_labels = tf.raw_ops.OneHot(indices=labels, depth=3, on_value=1, off_value=0, axis=-1).numpy().tolist()

def make_train_val_test(s1, s2, labels):
  np.random.seed(42)

  dataset = []
  max_len = len(s1)
  train_slice = int(0.8 * max_len)
  val_and_test_slice = int(0.1 * max_len)

  for i in range(max_len):
    dataset.append([s1[i], s2[i], labels[i]])

  dataset = np.array(dataset)
  np.random.shuffle(dataset)

  train = dataset[:int(0.8 * max_len)]
  val = dataset[int(0.8 * max_len):int(0.9 * max_len)]
  test = dataset[int(0.9 * max_len):]

  train_ds = tf.data.Dataset.from_tensor_slices(((np.array([e[0] for e in train]), np.array([e[1] for e in train])), np.array([e[2] for e in train])))
  val_ds = tf.data.Dataset.from_tensor_slices(((np.array([e[0] for e in val]), np.array([e[1] for e in val])), np.array([e[2] for e in val])))
  test_ds = tf.data.Dataset.from_tensor_slices(((np.array([e[0] for e in test]), np.array([e[1] for e in test])), np.array([e[2] for e in test])))

  return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = make_train_val_test(tokenized_s1, tokenized_s2, encoded_labels)

subset = train_ds.take(1)
for element in subset:
  print(f"{element[0][0]}, {type(element[0][0])}\n\n{element[0][1]}, {type(element[0][1])}\n\n{element[1]}, {type(element[1])}\n\n\n")

### BATCHSIZE ###

# batchsize = 8
# batchsize = 512
batchsize = 1024  #16gb
# batchsize = 2048
# batchsize = 4096  #40gb

train_ds = train_ds.batch(batchsize)
val_ds = val_ds.batch(batchsize)

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""---
# Create cmantique model :
"""

# class Cmantique(Model):
#   def __init__(self, bert=camembert, max_len=128):
#     super(Cmantique, self).__init__()
#     self.bert_model = bert
#     for layer in self.bert_model.layers:
#       layer.trainable = False

#     self.max_len = max_len
#     self.dense_intermediate = Dense(512)
#     self.dense_final = Dense(512, activation="tanh")
#     self.classifier = Dense(3)

#   def call(self, pair):
#     s1, s2 = pair

#     mask1 = tf.cast(tf.math.not_equal(s1, 0), tf.int32)
#     mask2 = tf.cast(tf.math.not_equal(s2, 0), tf.int32)

#     embeddings1 = self.bert_model(input_ids=s1, attention_mask=mask1)[0]
#     embeddings2 = self.bert_model(input_ids=s2, attention_mask=mask2)[0]

#     avg_embeddings1 = tf.math.reduce_mean(embeddings1, axis=1)
#     avg_embeddings2 = tf.math.reduce_mean(embeddings2, axis=1)

#     dense_intermediate_output1 = self.dense_intermediate(avg_embeddings1)
#     dense_intermediate_output2 = self.dense_intermediate(avg_embeddings2)
#     dense_final_output1 = self.dense_final(dense_intermediate_output1)
#     dense_final_output2 = self.dense_final(dense_intermediate_output2)

#     diff = tf.abs(dense_final_output1 - dense_final_output2)
#     classifier_output = self.classifier(diff)
#     return classifier_output

#   def get_sentence_embedding(self, sentence):
#     mask = tf.cast(tf.math.not_equal(sentence, 0), tf.int32)
#     embedding = self.bert_model(input_ids=sentence, attention_mask=mask)[0]
#     avg_embedding = tf.math.reduce_mean(embedding, axis=1)
#     dense_intermediate_output = self.dense_intermediate(avg_embedding)
#     dense_final_output = self.dense_final(dense_intermediate_output)
#     return dense_final_output

class Cmantique(Model):
  def __init__(self, bert=camembert, max_len=128):
    super(Cmantique, self).__init__()
    self.bert_model = bert
    for layer in self.bert_model.layers:
      layer.trainable = False

    self.max_len = max_len
    self.dense_intermediate1 = Dense(512)
    self.dense_intermediate2 = Dense(512)
    self.dense_final = Dense(512, activation="tanh")
    self.classifier = Dense(3)

  def call(self, pair):
    s1, s2 = pair

    mask1 = tf.cast(tf.math.not_equal(s1, 0), tf.int32)
    mask2 = tf.cast(tf.math.not_equal(s2, 0), tf.int32)

    embeddings1 = self.bert_model(input_ids=s1, attention_mask=mask1)[0]
    embeddings2 = self.bert_model(input_ids=s2, attention_mask=mask2)[0]

    avg_embeddings1 = tf.math.reduce_mean(embeddings1, axis=1)
    avg_embeddings2 = tf.math.reduce_mean(embeddings2, axis=1)

    dense_intermediate1_output1 = self.dense_intermediate1(avg_embeddings1)
    dense_intermediate1_output2 = self.dense_intermediate1(avg_embeddings2)
    dense_intermediate2_output1 = self.dense_intermediate2(dense_intermediate1_output1)
    dense_intermediate2_output2 = self.dense_intermediate2(dense_intermediate1_output2)
    dense_final_output1 = self.dense_final(dense_intermediate2_output1)
    dense_final_output2 = self.dense_final(dense_intermediate2_output2)

    diff = tf.abs(dense_final_output1 - dense_final_output2)
    classifier_output = self.classifier(diff)
    return classifier_output

  def get_sentence_embedding(self, sentence):
    mask = tf.cast(tf.math.not_equal(sentence, 0), tf.int32)
    embedding = self.bert_model(input_ids=sentence, attention_mask=mask)[0]
    avg_embedding = tf.math.reduce_mean(embedding, axis=1)
    dense_intermediate1_output = self.dense_intermediate1(avg_embedding)
    dense_intermediate2_output = self.dense_intermediate2(dense_intermediate1_output)
    dense_final_output = self.dense_final(dense_intermediate2_output)
    return dense_final_output

cmantique = Cmantique()

resume = Cmantique()
dummy_input = ([np.zeros((1, 1), dtype=np.int32)], [np.zeros((1, 1), dtype=np.int32)])
_ = resume(dummy_input)  # Exécutez une passe d'échantillonnage pour déterminer les formes des couches
resume.summary()  # Affichez le résumé du modèle

cmantique.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=["accuracy"])

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="./logs", histogram_freq=1)

cmantique.fit(train_ds, validation_data=val_ds, epochs=40, batch_size=batchsize, callbacks=[tensorboard_callback])

!zip -r ./logs_5.zip ./logs
files.download("./logs_6.zip")



"""---
# Tests :
"""

test = tokenizer.encode("test du la fonction du model.", return_tensors="tf")

cmantique.get_sentence_embedding(test).numpy().shape

















